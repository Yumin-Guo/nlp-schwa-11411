{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary Answer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrwpJ3TgyQJd"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words -= {'not'}\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "!pip install spacy\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import io\n",
        "import sys\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chns1n0k-30z"
      },
      "source": [
        "# 如果完全一致则为true\n",
        "# 如果任意方多了not则为false\n",
        "# 如果原文中词是hyper或synonym，则为true\n",
        "# 否则为false\n",
        "\n",
        "# or语句：拆成两句"
      ],
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyJUwWtFTwmv",
        "outputId": "4da3d611-9347-4e13-9559-677cc7315deb"
      },
      "source": [
        "def textToComponents(text):\n",
        "  doc = nlp(text)\n",
        "  subject_found, aux_found = False, False\n",
        "  subject, rest = None, None\n",
        "  prior = []\n",
        "  \n",
        "  for i, token in enumerate(doc):\n",
        "    # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_)\n",
        "    # print([t for t in token.head.subtree])\n",
        "    # print([t for t in token.subtree])\n",
        "\n",
        "    if subject_found and aux_found:\n",
        "      return [t.text for t in prior], [t.text for t in subject], [t.text for t in rest]\n",
        "    if not aux_found and (token.dep_ in {'aux', 'auxpass'} or token.pos_ == 'AUX'):\n",
        "      aux_found = True\n",
        "      rest = [t for t in doc[i+1:-1]]\n",
        "    elif not subject_found and token.dep_ in {'nsubj', 'nsubjpass'}:\n",
        "      subject_found = True\n",
        "      subject = [t for t in token.subtree]\n",
        "      prior = [t for t in prior if t not in token.subtree]\n",
        "    elif not subject_found and token.pos_ != 'PUNCT':\n",
        "      prior.append(token)\n",
        "  \n",
        "  # not found: no auxilary verb\n",
        "  if subject_found:\n",
        "    for i, token in enumerate(doc):\n",
        "      if token not in prior and token not in subject:\n",
        "        rest = doc[i:-1]\n",
        "        return [t.text for t in prior], [t.text for t in subject], [t.text for t in rest]\n",
        "\n",
        "def qToSentense(q, docT):\n",
        "  # Given a question and the processed declarative sentence, return the context, subject, rest of the sentence in the question\n",
        "  doc = nlp(q)\n",
        "  subject_found, aux_found = False, False\n",
        "  whole_sentence = []\n",
        "  prior_sentence = []\n",
        "  subject = []\n",
        "  rest_sentence = []\n",
        "  aux = None\n",
        "\n",
        "  skipTo = len(doc)\n",
        "  for i, token in enumerate(doc):\n",
        "    if not aux_found and (token.dep_ in {'aux', 'auxpass'} or token.pos_ == 'AUX'):\n",
        "      aux_found = True\n",
        "      aux = token\n",
        "    if not subject_found and token.dep_ in {'nsubj', 'nsubjpass'}:\n",
        "      subject_found = True\n",
        "      subject = [t for t in token.subtree]\n",
        "      prior_sentence = [w for w in prior_sentence if w not in subject]\n",
        "\n",
        "      skipTo = i + len(subject) - subject.index(token)\n",
        "      if skipTo >= len(doc)-1:\n",
        "        break # parsing is wrong\n",
        "      rest_sentence = doc[skipTo:-1]\n",
        "      # print(\" \".join([t.text for t in prior_sentence] + [t.text for t in subject] + [aux.text.lower()] + [t.text for t in rest_sentence]) + \".\")\n",
        "      return [t.text for t in prior_sentence], [t.text for t in subject], [t.text for t in rest_sentence]\n",
        "    if not aux_found and token.pos_ != 'PUNCT':\n",
        "      prior_sentence.append(token)\n",
        "  \n",
        "  # The case where nothing is found\n",
        "  possibleSubj = \" \".join(docT[1])\n",
        "  matching_subj = re.findall(r'\\b{phrase}\\b'.format(phrase=possibleSubj), q, re.IGNORECASE)\n",
        "  if len(matching_subj) > 0:\n",
        "    fixedSubj = nltk.word_tokenize(matching_subj[0])\n",
        "  else:\n",
        "    return None, None, None\n",
        "  for i, token in enumerate(doc):\n",
        "    if token not in prior_sentence and token != aux and token.pos_ != 'PUNCT' and token.text not in fixedSubj:\n",
        "      return [t.text for t in prior_sentence], fixedSubj, [t.text for t in doc[i:-1]]\n",
        "  return None, None, None\n",
        "\n",
        "sent = \"Was A third technique Hooper employed the off-centre framing of characters?\"\n",
        "text = \"A third technique Hooper employed was the off-centre framing of characters.\"\n",
        "\n",
        "# text = \"The weaving mill scene was filmed at the Queen Street Mill in Burnley.\"\n",
        "# sent = \"Was the weaving mill scene filmed at the King Street Mill in Burnley?\"\n",
        "\n",
        "print(textToComponents(text))\n",
        "print()\n",
        "print(qToSentense(sent, textToComponents(text)))\n",
        "\n"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([], ['A', 'third', 'technique', 'Hooper', 'employed'], ['the', 'off', '-', 'centre', 'framing', 'of', 'characters'])\n",
            "\n",
            "([], ['A', 'third', 'technique', 'Hooper', 'employed'], ['the', 'off', '-', 'centre', 'framing', 'of', 'characters'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiK6i3Awapry",
        "outputId": "24acda29-c3b7-4922-80da-955494474a67"
      },
      "source": [
        "def isHypernym(largeW, smallW):\n",
        "  print(largeW, smallW)\n",
        "  large = wn.synsets(largeW)\n",
        "  small = wn.synsets(smallW)\n",
        "  if large and small:\n",
        "    hypers = set()\n",
        "    for subsmall in small:\n",
        "      hypers = hypers.union(set([i for i in subsmall.closure(lambda s:s.hypernyms())]))\n",
        "    for sublarge in large:\n",
        "      if sublarge in hypers:\n",
        "        return True\n",
        "  return False\n",
        "\n",
        "def isSynonym(w1, w2):\n",
        "  print(w1, w2)\n",
        "  first = set(wn.synsets(w1))\n",
        "  second = set(wn.synsets(w2))\n",
        "  return len(first.intersection(second)) > 0\n",
        "\n",
        "def removeStopWords(wordL):\n",
        "  return [word.lower() for word in wordL if word.lower() not in stop_words]\n",
        "\n",
        "def subjectMatch(subjQ, subjT):\n",
        "  subjQ, subjT = removeStopWords(subjQ), removeStopWords(subjT)\n",
        "  notQ, notT = (\"not\" in subjQ), (\"not\" in subjT)\n",
        "  if notQ != notT:\n",
        "    return False\n",
        "  subjQ, subjT = \" \".join(subjQ), \" \".join(subjT)\n",
        "  return re.search(r'\\b{phrase}\\b'.format(phrase=subjQ), subjT) != None# or isHypernym(subjQ, subjT)\n",
        "\n",
        "def contextMatch(contextQ, contextT):\n",
        "  contextQ, contextT = removeStopWords(contextQ), removeStopWords(contextT)\n",
        "  contextQ, contextT = \" \".join(contextQ), \" \".join(contextT)\n",
        "  return not contextQ or re.search(r'\\b{phrase}\\b'.format(phrase=contextQ), contextT) != None\n",
        "\n",
        "def restMatch(restQ, restT):\n",
        "  restQ, restT = removeStopWords(restQ), removeStopWords(restT)\n",
        "  notQ, notT = (\"not\" in restQ), (\"not\" in restT)\n",
        "  if notQ != notT:\n",
        "    return False\n",
        "  for qw in restQ:\n",
        "    found = False\n",
        "    if qw in restT:\n",
        "      found = True\n",
        "    else:\n",
        "      print(\"\\t\\tTESTING HYPER...\", qw)\n",
        "      for tw in restT:\n",
        "        if isHypernym(qw, tw) or isSynonym(qw, tw):\n",
        "          found = True\n",
        "          break\n",
        "    if not found: \n",
        "      return False\n",
        "  return True\n",
        "\n",
        "text = \"Yesterday, George VI is captured hunched on the side of a couch at the edge of the frame.\"\n",
        "sent = \"Is George VI captured hunched on the side of a couch at the edge of the frame yesterday?\"\n",
        "docT = textToComponents(text)\n",
        "docQ = qToSentense(sent, docT)\n",
        "print(textToComponents(text), end = \"\\n\\n\")\n",
        "print(qToSentense(sent, docT), end = \"\\n\\n\")\n",
        "print(subjectMatch(docQ[1], docT[1]) and contextMatch(docQ[0], docT[0]) and restMatch(docQ[2], docT[2] + docT[0]))\n"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['Yesterday'], ['George', 'VI'], ['captured', 'hunched', 'on', 'the', 'side', 'of', 'a', 'couch', 'at', 'the', 'edge', 'of', 'the', 'frame'])\n",
            "\n",
            "([], ['George', 'VI'], ['captured', 'hunched', 'on', 'the', 'side', 'of', 'a', 'couch', 'at', 'the', 'edge', 'of', 'the', 'frame', 'yesterday'])\n",
            "\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66M88ZI3ctGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e1b456-e2a2-4e9c-fcb0-9eba9ba66645"
      },
      "source": [
        "def findSimilar(q): # should be handled somewhere in answer directly\n",
        "  return \"Seidler and Hooper were convinced of his suitability for the role.\"\n",
        "\n",
        "def answerBinary(q):\n",
        "  sentence = findSimilar(q)\n",
        "  docT = textToComponents(sentence)\n",
        "  # print(docT)\n",
        "  docQ = qToSentense(q, docT)\n",
        "  # print(docQ)\n",
        "  if docQ[1] and docQ[2]:\n",
        "    return subjectMatch(docQ[1], docT[1]) and contextMatch(docQ[0], docT[0]) and restMatch(docQ[2], docT[2] + docT[0])\n",
        "  return False\n",
        "\n",
        "q = \"Were Seidler and Hooper convinced of his suitability for the role?\"\n",
        "print(answerBinary(q))"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "XRwXgZNKaMAN",
        "outputId": "b9ef0f62-9be5-43f6-fc11-32550a3fd1f1"
      },
      "source": [
        "# Given a binary Q with keyword \"or\", assume only one word is altered, \n",
        "# check which question (after splitting) is true\n",
        "def splitOr(q):\n",
        "  words = nltk.word_tokenize(q)\n",
        "  if \"or\" in words: # must be true to call this function\n",
        "    index = words.index(\"or\")\n",
        "    q1 = words[:index] + words[index+2:-1]\n",
        "    q2 = words[:index-1] + words[index+1:-1]\n",
        "    return \" \".join(q1)+\"?\", \" \".join(q2)+\"?\", words[index-1], words[index+1]\n",
        "  else:\n",
        "    raise Exception(\"Question should contain 'or' to make this function valid\")\n",
        "\n",
        "def answerOr(q):\n",
        "  q1, q2, keyword1, keyword2 = splitOr(q)\n",
        "  if answerBinary(q1): \n",
        "    return keyword1[0].upper() + keyword1[1:] + \".\"\n",
        "  return keyword2[0].upper() + keyword2[1:] + \".\"\n",
        "\n",
        "answerOr(\"Is Tom good at painting or boxing?\")"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom is good at painting.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Boxing.'"
            ]
          },
          "metadata": {},
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpe0GM6KJCaV"
      },
      "source": [
        "**当前**：given the sentence, return True or False (probably not supported for do-lead questions) based on that\n",
        "\n",
        "to be combined with the wh-answer part to find the most relevant sentence\n",
        "\n",
        "**What else can be improved**: apposition"
      ]
    }
  ]
}
